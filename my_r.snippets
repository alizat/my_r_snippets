# ------------------------------------------------------------
# standard rstudio snippets
# ------------------------------------------------------------

snippet lib
	library(${1:package})

snippet req
	require(${1:package})

snippet src
	source("${1:file.R}")

snippet ret
	return(${1:code})

snippet mat
	matrix(${1:data}, nrow = ${2:rows}, ncol = ${3:cols})

snippet sg
	setGeneric("${1:generic}", function(${2:x, ...}) {
		standardGeneric("${1:generic}")
	})

snippet sm
	setMethod("${1:generic}", ${2:class}, function(${2:x, ...}) {
		${0}
	})

snippet sc
	setClass("${1:Class}", slots = c(${2:name = "type"}))

snippet if
	if (${1:condition}) {
		${0}
	}

snippet el
	else {
		${0}
	}

snippet ei
	else if (${1:condition}) {
		${0}
	}

snippet fun
	${1:name} <- function(${2:variables}) {
		${0}
	}

snippet for
	for (${1:variable} in ${2:vector}) {
		${0}
	}

snippet while
	while (${1:condition}) {
		${0}
	}

snippet switch
	switch (${1:object},
		${2:case} = ${3:action}
	)

snippet apply
	apply(${1:array}, ${2:margin}, ${3:...})

snippet lapply
	lapply(${1:list}, ${2:function})

snippet sapply
	sapply(${1:list}, ${2:function})

snippet mapply
	mapply(${1:function}, ${2:...})

snippet tapply
	tapply(${1:vector}, ${2:index}, ${3:function})

snippet vapply
	vapply(${1:list}, ${2:function}, FUN.VALUE = ${3:type}, ${4:...})

snippet rapply
	rapply(${1:list}, ${2:function})

snippet ts
	`r paste("#", date(), "------------------------------\n")`

snippet shinyapp
	library(shiny)
	
	ui <- fluidPage(
	  ${0}
	)
	
	server <- function(input, output, session) {
	  
	}
	
	shinyApp(ui, server)

snippet shinymod
	${1:name}UI <- function(id) {
	  ns <- NS(id)
		tagList(
			${2}
		)
	}
	
	${1:name}Server <- function(id) {
		moduleServer(id, function(input, output, session) {
			${3}
		})
	}

# ------------------------------------------------------------
# libraries
# ------------------------------------------------------------

snippet install.packages "packages to install in a new R environment"
	install.packages(
	  c(
	    ## data structures/wrangling/read/write/analysis
	    "tidyverse",
	    "lubridate",
	    "data.table",
	    "zeallot",
	    "readxl",
	    "dtplyr",
	    "naniar",
	    "mice",
	    "DataExplorer",
	    "skimr",
	    "rvest",
	    "glue",
	    "fst",
	    
	    # ## DB
	    # "odbc",
	    # "DBI",
	    # "RMySQL",
	    # "ROracle",
	    
	    ## shiny
	    "shiny",
	    "shinydashboard",
	    "shinymanager",
	    "shinyjs",
	    "flexdashboard",
	    "bslib", 
	    "showtext", 
	    "thematic",
	    "DT",
	    
	    ## maps
	    "leaflet",
	    "geosphere",
	    "osmdata",
	    "sf",
	    
	    ## data viz
	    "scales",
	    "plotly",
	    "patchwork",
	    "correlationfunnel",
	    "networkD3",
	    "ggiraph",
	    "dygraphs",
	    "rbokeh",
	    "echarts4r",
	    "canvasXpress",
	    "sunburstR",
	    "collapsibleTree",
	    "scatterD3",
	    "qtlcharts",
	    "d3Tree",
	    "radarchart",
	    "pairsD3",
	    "upsetjs",
	    "apexcharter",
	    "parcats",
	    "DiagrammeR",
	    "wordcloud",
	    
	    ## machine learning
	    "caret",
	    "mlr",
	    "ROCR",
	    "pROC",
	    "xgboost",
	    "randomForest",
	    "kernlab",
	    "e1071",
	    "party", 
	    "rpart",
	    "nnet",
	    
	    ## association analysis
	    "arules",
	    "arulesViz",
	    "arulesCBA", 
	    "arulesNBMiner", 
	    "arulesSequences",
	    
	    # ## rfm
	    # "rfm",
	    # "fuzzyjoin",
	    
	    ## software engineering
	    "httr",
	    "jsonlite",
	    
	    ## miscellaneous
	    "tictoc"
	  ),
	  repo = "http://cran.rstudio.com/"
	  # repo = "https://cloud.r-project.org"
	  # repo = "http://cran.us.r-project.org"
	)

snippet install_version "install specific version of a library"
	devtools::install_version(${1:ggplot2} version = ${2:0.9.1}, repos = "http://cran.us.r-project.org")

snippet install_pkg_from_source "install package from local file"
	install.packages(${1:path_to_file}, repos = NULL, type="source")

snippet libs "my favorite libraries"
	suppressMessages({
		library(tidyverse)
		library(lubridate)
		library(plotly)
		library(tictoc)
		library(glue)
		
		options(dplyr.summarise.inform = FALSE)
	})

snippet libs4shiny "libraries to use in shiny apps"
	suppressMessages({
		library(tidyverse)
		library(lubridate)
		library(plotly)
		library(tictoc)
		library(glue)
		
		library(shiny)
		library(leaflet)
		library(DT)
		
		options(dplyr.summarise.inform = FALSE)
	})

snippet tv "load tidyverse"
	library(tidyverse)

snippet tvall "load tidyverse individual libraries"
	library(ggplot2)
	library(tibble)
	library(tidyr)
	library(readr)
	library(purrr)
	library(dplyr)
	library(stringr)
	library(forcats)
	library(lubridate)
	library(glue)
	library(fs)

# ------------------------------------------------------------
# operators
# ------------------------------------------------------------

snippet op:default
	# use y if x is.null
	`%||%` <- function(x, y) if (is.null(x)) y else x

snippet op:notnull
	# use y if x is not null (otherwise NULL)
	`%??%` <- function(x, y) if (!is.null(x)) y

snippet op:notnullish
	# use y if x is not null(ish) (otherwise NULL)
	`%??%` <- function(x, y) if (!is.null(x) && x != "") y

# ------------------------------------------------------------
# general
# ------------------------------------------------------------

snippet prettydate
	strftime(Sys.time(), "%A, %b %d, %Y")

snippet hdr
	## ---- ${1:header} ----

snippet subhdr
	#### ---- ${1:subhdr} ----

snippet subsubhdr
	######## ---- ${1:subsubhdr} ----

snippet ie
	if (${1:cond}) ${2:true} else ${3:false}

# ------------------------------------------------------------
# ggplot2
# the following snippets are intended to be used as follows:
#   data %>% <snippet>
# ------------------------------------------------------------

snippet gg "ggplot generic"
	ggplot(aes(${2:aes})) + ${0}

snippet gl "ggplot line"
	ggplot(aes(${2:x}, ${3:y})) + geom_line()${0}

snippet gp "ggplot point"
	ggplot(aes(${2:x}, ${3:y})) + geom_point()${0}

snippet gb "ggplot bar"
	count(${2:var}) %>% 
	  ggplot(aes(${2:var}, n)) + 
	  geom_bar(stat = 'identity')

snippet ggsave "save plot to PDF"
	ggsave("${1:filename}.pdf", width = ${2:6}, height = ${3:6})${0}

snippet gglegendhide "hide the legend"
	theme(legend.position = 'none')

snippet ggscalecontinuousnormal "non-scientific tick values"
	scale_x_continuous(labels = scales::comma)

snippet ge "geom_*"
	geom_${1:point}(${0})

snippet ggeb "element_blank"
	element_blank($0)

snippet gger "element_rect"
	element_rect($0)

snippet gget "element_text"
	element_text($0)

snippet ggel "element_line"
	element_line($0)

# ------------------------------------------------------------
# visualization
# ------------------------------------------------------------

snippet corrfunnel "correlation funnel visualization"
	library(correlationfunnel)
	${1:iris} %>%
		drop_na() %>%
		binarize(n_bins = 6) %>%
		correlate(${2:Species__setosa}) %>%
		plot_correlation_funnel(interactive = TRUE, alpha = 0.7)

snippet saveplotly
	as_widget() %>% htmlwidgets::saveWidget("${1:filename.html}")

# ------------------------------------------------------------
# wrangling
# the following snippets are intended to be used as follows:
#   data %>% <snippet>
# ------------------------------------------------------------

snippet pivot_wider
	pivot_wider(names_from = ${1:names_col}, values_from = ${2:values_col}, values_fill = ${3:0})

snippet pivot_longer
	pivot_longer(cols = ${1:my_cols}, names_to = '${2:names_col}', values_to = '${3:values_col}')

# ------------------------------------------------------------
# read/write
# the following snippets are intended to be used as follows:
#   data %>% <snippet>
# ------------------------------------------------------------

snippet read_csv
	read_csv('${1:filename}', show_col_types = FALSE${2:, col_types = cols(.default = 'c')})

snippet read_tsv
	read_tsv('${1:filename}', show_col_types = FALSE${2:, col_types = cols(.default = 'c')})

snippet read_delim
	read_delim('${1:filename}', delim = '${|}', show_col_types = FALSE${3:, col_types = cols(.default = 'c')})

snippet read_fst
	read_fst('${1:filename}')

snippet xlsxread "read all sheets of an xlsx file into a list"
	read_entire_xlsx <- function(xlsx_file) {
	  all_sheets <- readxl::excel_sheets(xlsx_file)
	  all_sheets <- 
	  	all_sheets %>% 
	  	map(~ readxl::read_xlsx(xlsx_file, sheet = .x) %>% mutate_all(as.character)) %>% 
	  	set_names(all_sheets)
	  if (length(all_sheets) == 1) all_sheets[[1]] else all_sheets
	}
	stuff <- read_entire_xlsx('${1:file}')

snippet write_csv
	write_csv('${1:filename}')

snippet write_fst
	write_fst('${1:filename}')

snippet write_xlsx
	writexl::write_xlsx('${1:filename}')

# ------------------------------------------------------------
# environment
# ------------------------------------------------------------

snippet arabiclocaleenv "Arabic characters to appear fine in RStudio console (Windows)"
	Sys.setlocale(locale = "Arabic")

snippet arabiclocaleparam "locale parameters in read_* functions of readr package"
	locale = locale(encoding = "Windows-1256")

snippet lsobjects "get sizes of variables in environment"
	.ls.objects <- function (pos = 1, pattern, order.by = "Size", decreasing=TRUE, head = TRUE, n = 10) {
	    # based on postings by Petr Pikal and David Hinds to the r-help list in 2004
	    # modified by: Dirk Eddelbuettel (http://stackoverflow.com/questions/1358003/tricks-to-manage-the-available-memory-in-an-r-session) 
	    # I then gave it a few tweaks (show size as megabytes and use defaults that I like)
	    # a data frame of the objects and their associated storage needs.
	    napply <- function(names, fn) sapply(names, function(x)
	        fn(get(x, pos = pos)))
	    names <- ls(pos = pos, pattern = pattern)
	    obj.class <- napply(names, function(x) as.character(class(x))[1])
	    obj.mode <- napply(names, mode)
	    obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
	    obj.size <- napply(names, object.size) / 10^6 # megabytes
	    obj.dim <- t(napply(names, function(x)
	        as.numeric(dim(x))[1:2]))
	    vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
	    obj.dim[vec, 1] <- napply(names, length)[vec]
	    out <- data.frame(obj.type, obj.size, obj.dim)
	    names(out) <- c("Type", "Size", "Rows", "Columns")
	    out <- out[order(out[[order.by]], decreasing=decreasing), ]
	    if (head)
	        out <- head(out, n)
	    out
	}

snippet object_size "get size of specific variables in environment"
	pryr::object_size(iris, mtcars)

# ------------------------------------------------------------
# machine learning
# ------------------------------------------------------------

snippet xgbmodel_reg
	dta_to_build_model_with <- ${1:data}
	target_variable <- '${2:targetvar}'
	
	library(xgboost)
	library(pROC)
	
	set.seed(1023154)
	num_rows <- nrow(dta_to_build_model_with)
	randomized <- sample(num_rows)
	df_train <- dta_to_build_model_with[randomized[1:floor(0.6*num_rows)],]
	df_val   <- dta_to_build_model_with[randomized[(floor(0.6*num_rows) + 1):floor(0.8*num_rows)],]
	df_test  <- dta_to_build_model_with[randomized[(floor(0.8*num_rows) + 1):num_rows],]
	
	hypergrid <- expand_grid(eta       = seq(0.1, 0.9, 0.2),
	                         max_depth = seq(2, 8, 2),
	                         nrounds   = seq(10, 50, 10), 
	                         auc       = 0, 
	                         avg_error = 0, 
	                         med_error = 0)
	print(glue('Hyperparameter tuning begins...'))
	best_val_AUC         <- -1
	best_val_predictions <- NULL
	for (i in 1:nrow(hypergrid)) {
	    xgb <- xgboost(data      = df_train %>% select(-all_of(target_variable)) %>% data.matrix,
	                   label     = df_train[[target_variable]],
	                   eta       = hypergrid[['eta']][i],
	                   max_depth = hypergrid[['max_depth']][i],
	                   nrounds   = hypergrid[['nrounds']][i],
	                   objective = "reg:squarederror",  ## 'binary:logistic' if classification
	                   # eval_metric = 'auc',  ## uncomment this line if classification
	                   verbose   = 0)
	    predictions <- predict(xgb, data.matrix(df_val %>% select(-all_of(target_variable))))
	    
	    ## compute area under ROC curve
	    actual <- df_val[[target_variable]]
	    suppressMessages({ hypergrid[['auc']][i] <- auc(roc(actual, predictions)) %>% as.numeric() })
	    
	    print_me <- glue("[{now()}]")
	    print_me <- glue("[{, print_me}eta={hypergrid[['eta']][i]}")
	    print_me <- glue("[{, print_me}max_depth={hypergrid[['max_depth']][i]}")
	    print_me <- glue("[{, print_me}nrounds={hypergrid[['nrounds']][i]}")
	    print_me <- glue("[{, print_me}auc={hypergrid[['auc']][i]}")
	    
	    ## regression only
	    hypergrid[['avg_error']][i] <-   mean(abs(predictions - df_val[[target_variable]]) / df_val[[target_variable]], na.rm = TRUE)
	    hypergrid[['med_error']][i] <- median(abs(predictions - df_val[[target_variable]]) / df_val[[target_variable]], na.rm = TRUE)
	    print_me <- glue("[{, print_me}avg_error={round(hypergrid[['avg_error']][i] * 100, 1)}%")
	    print_me <- glue("[{, print_me}med_error={round(hypergrid[['med_error']][i] * 100, 1)}%")
	    
	    print(print_me)
	    
	    if (best_val_AUC < hypergrid[['auc']][i]) {
	      best_val_AUC         <- hypergrid[['auc']][i]
	      best_val_predictions <- val_predictions
	    }
	}
	print(glue('Hyperparameter tuning ends...'))
	best_params <- 
	    hypergrid %>% 
	    arrange(desc(auc)) %>% 
	    head(1) %>% 
	    as.list()
	
	print(glue('Prediciting on test set...'))
	xgb <- xgboost(data      = df_train %>% rbind(df_val) %>% select(-all_of(target_variable)) %>% data.matrix,
	               label     = (df_train %>% rbind(df_val))[[target_variable]],
	               eta       = best_params[['eta']][i],
	               max_depth = best_params[['max_depth']][i],
	               nrounds   = best_params[['nrounds']][i],
	               objective = "reg:squarederror",  ## 'binary:logistic' if classification
	               # eval_metric = 'auc',  ## uncomment this line if classification
	               verbose   = 0)
	predictions <- predict(xgb, data.matrix(df_test %>% select(-all_of(target_variable))))
	
	## compute area under ROC curve
	actual <- df_val[[target_variable]]
	suppressMessages({ hypergrid[['auc']][i] <- auc(roc(actual, predictions)) %>% as.numeric() })
	
	print(glue("    eta       = {best_params[['eta']][i]}"))
	print(glue("    max_depth = {best_params[['max_depth']][i]}"))
	print(glue("    nrounds   = {best_params[['nrounds']][i]}"))
	print(glue("    auc       = {best_params[['auc']][i]}"))
	
	## regression only
	avg_error_test <-   mean(abs(predictions - df_test[[target_variable]]) / df_test[[target_variable]], na.rm = TRUE)
	med_error_test <- median(abs(predictions - df_test[[target_variable]]) / df_test[[target_variable]], na.rm = TRUE)
	print(glue("    avg_error = {best_params[['avg_error']][i]}%"))
	print(glue("    med_error = {best_params[['med_error']][i]}%"))
	
	## append details of model training
	stuff <- list()
	stuff[['mdl']] <- xgb
	stuff[['hypergrid']] <- hypergrid
	stuff[['best_params']] <- best_params
	stuff[['val_pred_vs_actual']] <- 
	    tibble(pred = best_val_predictions,
	           actual = df_val[[target_variable]])
	histogram_plotter <- function(pred_vs_actual) {
	  pred_vs_actual %>% 
	    ggplot(aes(x = pred, fill = actual_aux)) + 
	    geom_histogram(binwidth = 0.01, 
	                   position = 'identity', 
	                   alpha = 0.5)
	}
	stuff[['val_histogram_plot']] <- histogram_plotter(stuff)
	# stuff[['val_ct']] <- coverager(preds = best_val_predictions, actual_binary = df_val[[target_variable]])
	
	# ## append the inputs that were given to this function
	# stuff[['train_set']]               <- train_set
	# stuff[['validation_set']]          <- validation_set
	# stuff[['explanatory_variables']]   <- explanatory_variables
	# stuff[['target_variable']]         <- target_variable
	# stuff[['target_variable_mapping']] <- target_variable_mapping

# ------------------------------------------------------------
# shiny
# ------------------------------------------------------------

snippet shinyauth "add authentication to a shiny app"
	library(shiny)
	${1:library(shinymanager)}
	
	${2:credentials <-
			data.frame(user = c("shiny", "shinymanager"),
					   password = c("azerty", "12345"),
					   admin = c(T, F),
					   stringsAsFactors = FALSE)}
	
	ui <- fluidPage()
	
	${3:ui <- secure_app(ui, enable_admin = T)}
	
	server <- function(input, output) {
	
		${4:secure_server(check_credentials = check_credentials(credentials))}
		
	}
	
	shinyApp(ui = ui, server = server)

snippet selectInput
	selectInput(
		inputId  = '${1:my_select_input}', 
		label    = '${2:Select...}', 
		choices  = c(${3:'choice 1', 'choice 2'}) %>% set_names(str_to_title(.)), 
		selected = ${4:'choice 1'}, 
		multiple = ${5:FALSE}
	)

snippet sliderInput
	sliderInput(
		inputId = '${1:my_slider_input}', 
		label   = '${2:Select...}', 
		min     = ${3:0}, 
		max     = ${4:100}, 
		value   = ${5:50}
	)

snippet numericInput
	numericInput(
		inputId = '${1:my_numeric_input}', 
		label   = '${2:Select...}', 
		min     = ${3:0}, 
		max     = ${4:100}, 
		value   = ${5:50}
	)

snippet radioButtons
	radioButtons(
		inputId  = '${1:my_numeric_input}', 
		label    = '${2:Select...}', 
		choices  = c(${3:'choice 1', 'choice 2'}) %>% set_names(str_to_title(.)), 
		selected = ${4:'choice 1'}
	)

snippet datatable
	display_me = ${1:data_df}
	colnames(display_me) <- 
		colnames(display_me) %>% 
		str_split('_') %>% 
		map_chr(~ .x %>% unlist() %>% paste(collapse = ' ')) %>% 
		str_to_title()
	# display_me <- 
	# 	display_me %>% 
	# 	mutate_if(is.numeric, round, digits = 1)
	display_me %>% 
		datatable(
			filter = 'top',
			rownames = FALSE,
			options = list(
				dom = 'ltip',
				columnDefs = list(
					list(className = 'dt-center', targets = '_all'),
					list(width = paste0(100 / ncol(display_me), '%'), targets = '_all')
				)
			)
		) %>% 
		formatStyle(1:ncol(display_me), lineHeight = '40%') %>% 
		formatRound(colnames(display_me)[display_me %>% map_lgl(is.numeric)], digits=1)

# ------------------------------------------------------------
# parallelism
# ------------------------------------------------------------

snippet prlll
	${1:data_out} <- ${2:data_in} %>% data.table::as.data.table()
	ggg <- gc()  ## garbage collection (silent)
	
	split_with_these <- c('${3:splitvars}')
	split_plan <- 
		${1:data_out} %>% 
		select(all_of(split_with_these)) %>% 
		distinct() %>% 
		mutate(grouping = sample(1000, nrow(.), replace = TRUE))
	
	${1:data_out} <- 
		${1:data_out} %>% 
		inner_join(split_plan, by = split_with_these)
	
	${1:data_out} <- ${1:data_out} %>% split(by = 'grouping')
	ggg <- gc()  ## garbage collection (silent)
	
	## prepare for parallelization
	library(parallel)
	num_cores_total <- detectCores()
	num_cores_to_use <- min(floor(num_cores_total * 0.75) - 1, nrow(split_plan))
	# print('# cores to use:' %>% paste(num_cores_to_use))
	clust <- makeCluster(getOption("cl.cores", num_cores_to_use))
	
	## PARALLELISM !!!
	${1:data_out} <- parLapplyLB(clust, ${1:data_out}, function(df) {
		library(tidyverse)
		library(lubridate)
		
		# ## stuff you wanna do!
		# df <- df %>% <do something here!!!>
		
		## return processed result
		df
	})
	
	## 10 seconds pause after above parallel computation
	Sys.sleep(time = ${4:10})
	
	## clean up
	stopCluster(clust)
	
	## merge results of the different cores
	${1:data_out} <- ${1:data_out} %>% data.table::rbindlist()
	
	## remove 'grouping' field (if still exists)
	${1:data_out} <- ${1:data_out} %>% select(-any_of('grouping'))

# ------------------------------------------------------------
# summaries
# ------------------------------------------------------------

snippet skm
	skimr::skim_without_charts(${1:dplyr::storms}) %>% 
	#arrange(n_missing, character.n_unique, factor.n_unique, sd)
	do({
	  df <- .
	  
	  df <- df %>% arrange(n_missing)
	  
		classes <- df %>% sapply(class) %>% unlist() %>% unique()
		
		if (any(classes %in% c('character','factor','ordered','Date','POSIXct','POSIXt'))) {
			arrange_with_these <- colnames(df)[colnames(df) %>% str_detect('unique')]
			df <- df %>% arrange(across(arrange_with_these))
		}
			
		if (any(classes %in% c('numeric','integer','double'))) {
			arrange_with_these <- colnames(df)[colnames(df) %>% str_detect('sd')]
			df <- df %>% arrange(across(arrange_with_these))
		}
		
		df
	})

snippet grp_cnt
	dta_to_count <- ${1:data}
	grpvars <- c('${2:groupvars}')
	dta_to_count %>% 
		group_by_at(all_of(grpvars)) %>% 
		summarise(n = n(), .groups = 'drop_last') %>% 
		ungroup()

snippet grp_qtiles
	dta_to_quantile <- ${1:data}
	grpvars <- c('${2:groupvars}')
	p0   <- function(x) {min(x, na.rm = TRUE)}
	p5   <- function(x) {quantile(x, probs = 0.05, na.rm = TRUE)}
	p25  <- function(x) {quantile(x, probs = 0.25, na.rm = TRUE)}
	p50  <- function(x) {quantile(x, probs = 0.50, na.rm = TRUE)}
	p75  <- function(x) {quantile(x, probs = 0.75, na.rm = TRUE)}
	p95  <- function(x) {quantile(x, probs = 0.95, na.rm = TRUE)}
	p100 <- function(x) {max(x, na.rm = TRUE)}
	ps <- list(MIN = p0, 
	           p5  = p5, 
	           p25 = p25, 
	           p50 = p50, 
	           p75 = p75, 
	           p95 = p95, 
	           MAX = p100, 
	           n = length)
	display_me <-
		dta_to_quantile %>% 
		mutate(super_unique_column = 1) %>% 
		# lazy_dt() %>% 
		group_by(across(all_of(grpvars))) %>% 
		summarise_if(is.numeric, .funs = ps) %>% 
		ungroup() %>% 
		select(-starts_with('super_unique_column')) %>% 
		# as_tibble() %>% 
	    pivot_longer(cols = (length(grpvars) + 1):ncol(.),
	                 names_to = c('feature', 'qtile'),
	                 names_pattern = '(.*)_(.*)', 
	                 values_to = 'val') %>% 
	    mutate(qtile = qtile %>% factor(levels = names(ps))) %>%
	    pivot_wider(names_from = feature, values_from = val)
	silence <- 
	    display_me %>% 
	    group_by_at(all_of(grpvars)) %>% 
	    do({
	        my_stuff <- .
	        
	        print(glue(''))
	        for (i in 1:length(grpvars)) {
	            print(glue('{grpvars[i]} = {my_stuff %>% select(all_of(grpvars[i])) %>% pull() %>% unique()}'))
	        }
	        print(glue('Number of Observations = {my_stuff %>% filter(qtile == "n") %>% select(-qtile, -all_of(grpvars)) %>% unlist() %>% tail(1)}'))
	        my_stuff %>% filter(qtile != 'n') %>% print()
	        print(glue(''))
	        
	        my_stuff
	    }) %>% 
	    ungroup()

snippet grp_summary
	dta_to_summarize <- ${1:data}
	grpvars <- c('${2:groupvars}')
	silence <- 
	    dta_to_summarize %>% 
	    group_by(across(all_of(grpvars))) %>% 
	    do({
	        my_stuff <- .
	        
	        print(glue(''))
	        for (i in 1:length(grpvars)) {
	            print(glue('{grpvars[i]} = {my_stuff %>% select(all_of(grpvars[i])) %>% pull() %>% unique()}'))
	        }
	        print(glue('Number of Observations = {my_stuff %>% nrow()}'))
	        print(my_stuff %>% select(-all_of(grpvars)) %>% summary())
	        print(glue(''))
	        
	        my_stuff
	    }) %>% 
	    ungroup()

snippet grp_smr
	${1:data} %>% 
	  group_by(${2:grpvar}) %>% 
	  summarise(${3:across(everything(), mean)}) %>% 
	  ungroup()
